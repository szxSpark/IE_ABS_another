import torch.nn as nn

class DecInit(nn.Module):
    def __init__(self, opt):
        super(DecInit, self).__init__()
        # self.num_directions = 2 if opt.brnn else 1
        # assert opt.enc_rnn_size % self.num_directions == 0
        # self.enc_rnn_size = opt.enc_rnn_size
        self.enc_rnn_size = 512

        self.dec_rnn_size = opt.dec_rnn_size
        self.initer = nn.Linear(self.enc_rnn_size, self.dec_rnn_size)

        self.tanh = nn.Tanh()

    def forward(self, last_enc_h):
        # last_enc_h, (B, H)
        # batchSize = last_enc_h.size(0)
        # dim = last_enc_h.size(1)
        return self.tanh(self.initer(last_enc_h))  # 为什么
